{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjf0S2yqD0uf"
   },
   "source": [
    "# Stock Price Prediction Using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhNGhRQWjN94"
   },
   "source": [
    "## Objective\n",
    "The objective of this assignment is to try and predict the stock prices using historical data from four companies IBM (IBM), Google (GOOGL), Amazon (AMZN), and Microsoft (MSFT).\n",
    "\n",
    "We use four different companies because they belong to the same sector: Technology. Using data from all four companies may improve the performance of the model. This way, we can capture the broader market sentiment.\n",
    "\n",
    "The problem statement for this assignment can be summarised as follows:\n",
    "\n",
    "> Given the stock prices of Amazon, Google, IBM, and Microsoft for a set number of days, predict the stock price of these companies after that window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4gU_Gs--yjG"
   },
   "source": [
    "## Business Value\n",
    "\n",
    "Data related to stock markets lends itself well to modeling using RNNs due to its sequential nature. We can keep track of opening prices, closing prices, highest prices, and so on for a long period of time as these values are generated every working day. The patterns observed in this data can then be used to predict the future direction in which stock prices are expected to move. Analyzing this data can be interesting in itself, but it also has a financial incentive as accurate predictions can lead to massive profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC424ieDIlaK"
   },
   "source": [
    "### **Data Description**\n",
    "\n",
    "You have been provided with four CSV files corresponding to four stocks: AMZN, GOOGL, IBM, and MSFT. The files contain historical data that were gathered from the websites of the stock markets where these companies are listed: NYSE and NASDAQ. The columns in all four files are identical. Let's take a look at them:\n",
    "\n",
    "- `Date`: The values in this column specify the date on which the values were recorded. In all four files, the dates range from Jaunary 1, 2006 to January 1, 2018.\n",
    "\n",
    "- `Open`: The values in this column specify the stock price on a given date when the stock market opens.\n",
    "\n",
    "- `High`: The values in this column specify the highest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Low`: The values in this column specify the lowest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Close`: The values in this column specify the stock price on a given date when the stock market closes.\n",
    "\n",
    "- `Volume`: The values in this column specify the total number of shares traded on a given date.\n",
    "\n",
    "- `Name`: This column gives the official name of the stock as used in the stock market.\n",
    "\n",
    "There are 3019 records in each data set. The file names are of the format `\\<company_name>_stock_data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzV7hepM8K-H"
   },
   "source": [
    "## **1 Data Loading and Preparation** <font color =red> [25 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6UQ2zyxH1Ep"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RyZsIlDgfO3s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# These are the basic libraries we imported. Other libraries we will import as and when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uM6lg_TMNbJp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4_Xnhwsl-00"
   },
   "source": [
    "### **1.1 Data Aggregation** <font color =red> [7 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5clrTkdAvq8G"
   },
   "source": [
    "As we are using the stock data for four different companies, we need to create a new DataFrame that contains the combined data from all four data frames. We will create a function that takes in a list of the file names for the four CSV files, and returns a single data frame. This function performs the following tasks:\n",
    "- Extract stock names from file names\n",
    "- Read the CSV files as data frames\n",
    "- Append the stock names into the columns of their respective data frames\n",
    "- Drop unnecessary columns\n",
    "- Join the data frames into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tPMCwNwe1JS"
   },
   "source": [
    "#### **1.1.1** <font color =red> [5 marks] </font>\n",
    "Create the function to join DataFrames and use it to combine the four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-gj0K2Q65yix",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to load data and aggregate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5tFT0P9bNbJq",
    "outputId": "47b7067a-09a6-4231-e643-9fbbda427aea",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\andhondi\\\\OneDrive - Cisco\\\\dhondi_a\\\\MS-AIML\\\\Recurrent Neural Networks Assignment\\\\RNN_Stock_Starter_Dataset\\\\RNN_Stock_Starter_Dataset\\\\RNN_Stocks_Data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the current working directory:\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r0sYevlt7kgn",
    "outputId": "ff4f8e1f-434f-4a89-adc7-4cf0ec9a5daf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMZN_stocks', 'GOOGL_stocks', 'IBM_stocks', 'MSFT_stocks']\n"
     ]
    }
   ],
   "source": [
    "# Specify the names of the raw data files to be read and use the aggregation function to read the files\n",
    "\n",
    "# Reading the stock files names:\n",
    "file_names = []\n",
    "for files in os.listdir(os.getcwd()):\n",
    "    if files.endswith('.csv'):\n",
    "        file_names.append(files[:-9])\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Msav-DjINbJr",
    "outputId": "c70bcc17-6056-4164-fc4a-0996b56f225e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMZN_stocks_data.csv', 'GOOGL_stocks_data.csv', 'IBM_stocks_data.csv', 'MSFT_stocks_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# Reading the stock files names:\n",
    "files_csv = []\n",
    "for files in os.listdir(os.getcwd()):\n",
    "    if files.endswith('.csv'):\n",
    "        files_csv.append(files)\n",
    "print(files_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fwQesOeoNbJr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenating all the csv files into one dataframe and by date:\n",
    "\n",
    "def mergedfbydate(files_csv):\n",
    "\n",
    "    mergeddf = None # empty variable\n",
    "    for stock in files_csv:\n",
    "        df = pd.read_csv(stock)\n",
    "\n",
    "        stock_name = stock.split('_')[0]\n",
    "        df = df[['Date','Close']].copy() # using copy since copy enusures returning a new dataframe and not a view\n",
    "        df.rename(columns={'Close':stock_name}, inplace=True)\n",
    "\n",
    "        if mergeddf is None:\n",
    "            mergeddf = df\n",
    "        else:\n",
    "            mergeddf = pd.merge(mergeddf, df, on='Date', how = 'outer')\n",
    "\n",
    "    # converting date from string to datetime:\n",
    "    mergeddf['Date'] = pd.to_datetime(mergeddf['Date'])\n",
    "    # sorting based on date to create sequence later:\n",
    "    mergeddf = mergeddf.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    return mergeddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G_VoEiV3NbJr",
    "outputId": "dddb9646-8091-4d22-caf8-a937df618391",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date   AMZN   GOOGL    IBM   MSFT\n",
      "0 2006-01-03  47.58  217.83  82.06  26.84\n",
      "1 2006-01-04  47.25  222.84  81.95  26.97\n",
      "2 2006-01-05  47.65  225.85  82.50  26.99\n",
      "3 2006-01-06  47.87  233.06  84.95  26.91\n",
      "4 2006-01-09  47.08  233.68  83.73  26.86\n"
     ]
    }
   ],
   "source": [
    "combined_df = mergedfbydate(files_csv)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4qMKdhGj8cKF",
    "outputId": "a087ac48-695b-4927-a63a-b13516b0180d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3020 entries, 0 to 3019\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   Date    3020 non-null   datetime64[ns]\n",
      " 1   AMZN    3019 non-null   float64       \n",
      " 2   GOOGL   3019 non-null   float64       \n",
      " 3   IBM     3020 non-null   float64       \n",
      " 4   MSFT    3019 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 118.1 KB\n"
     ]
    }
   ],
   "source": [
    "# View specifics of the data\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MqpfHSGNNbJr",
    "outputId": "fb96fea5-fdfb-4b0a-95cc-af40eec8a866",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3020</td>\n",
       "      <td>3019.000000</td>\n",
       "      <td>3019.000000</td>\n",
       "      <td>3020.000000</td>\n",
       "      <td>3019.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2012-01-01 14:54:59.602649088</td>\n",
       "      <td>299.376231</td>\n",
       "      <td>428.044001</td>\n",
       "      <td>145.617278</td>\n",
       "      <td>36.513412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2006-01-03 00:00:00</td>\n",
       "      <td>26.070000</td>\n",
       "      <td>128.850000</td>\n",
       "      <td>71.740000</td>\n",
       "      <td>15.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2009-01-01 12:00:00</td>\n",
       "      <td>81.090000</td>\n",
       "      <td>247.605000</td>\n",
       "      <td>116.525000</td>\n",
       "      <td>26.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2011-12-29 12:00:00</td>\n",
       "      <td>205.440000</td>\n",
       "      <td>310.080000</td>\n",
       "      <td>149.315000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>375.140000</td>\n",
       "      <td>570.770000</td>\n",
       "      <td>178.685000</td>\n",
       "      <td>44.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017-12-29 00:00:00</td>\n",
       "      <td>1195.830000</td>\n",
       "      <td>1085.090000</td>\n",
       "      <td>215.800000</td>\n",
       "      <td>86.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>279.980161</td>\n",
       "      <td>236.343238</td>\n",
       "      <td>37.529387</td>\n",
       "      <td>14.694656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date         AMZN        GOOGL          IBM  \\\n",
       "count                           3020  3019.000000  3019.000000  3020.000000   \n",
       "mean   2012-01-01 14:54:59.602649088   299.376231   428.044001   145.617278   \n",
       "min              2006-01-03 00:00:00    26.070000   128.850000    71.740000   \n",
       "25%              2009-01-01 12:00:00    81.090000   247.605000   116.525000   \n",
       "50%              2011-12-29 12:00:00   205.440000   310.080000   149.315000   \n",
       "75%              2014-12-31 12:00:00   375.140000   570.770000   178.685000   \n",
       "max              2017-12-29 00:00:00  1195.830000  1085.090000   215.800000   \n",
       "std                              NaN   279.980161   236.343238    37.529387   \n",
       "\n",
       "              MSFT  \n",
       "count  3019.000000  \n",
       "mean     36.513412  \n",
       "min      15.150000  \n",
       "25%      26.835000  \n",
       "50%      30.000000  \n",
       "75%      44.400000  \n",
       "max      86.850000  \n",
       "std      14.694656  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM0qzp3XNbJr",
    "outputId": "26bfbe49-c470-492d-a045-eeff1b29e3bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df.describe().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwVzhIuBfRGn"
   },
   "source": [
    "#### **1.1.2** <font color =red> [2 marks] </font>\n",
    "Identify and handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vmQGhf69x36",
    "outputId": "43a14e8a-2328-48a4-edec-6d61ba73fc7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "print('Column-wise missing values:\\n', '\\n', combined_df.isnull().sum()) # Only few missing values --> lets drop them\n",
    "print(' ')\n",
    "print('Total number of missing values:',combined_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_UKx2KUNbJs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets drop the missing values:\n",
    "combined_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckJbD8V2NbJs",
    "outputId": "b41a4022-680e-4078-801a-792fb39cccd9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropped all the Missing Values:\n",
    "\n",
    "print('Column-wise missing values:\\n', '\\n', combined_df.isnull().sum()) # Only few missing values --> lets drop them\n",
    "print(' ')\n",
    "print('Total number of missing values:',combined_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuFFxr5jQ4xw"
   },
   "source": [
    "### **1.2 Analysis and Visualisation** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnybVo_YQ4xx"
   },
   "source": [
    "#### **1.2.1** <font color =red> [2 marks] </font>\n",
    "Analyse the frequency distribution of stock volumes of the companies and also see how the volumes vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBZSQis3NbJs",
    "outputId": "b04dadfc-510e-48d9-ba8f-ff3a60ce7312",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this instead of 'Close' we need to use the 'Volume' column to make the combined dataframe. Lets do that first.\n",
    "# We will follow the same steps. Only difference will be the column name.\n",
    "\n",
    "# concatenating all the csv files into one dataframe and by date:\n",
    "\n",
    "def mergedfbydate_volume(files_csv):\n",
    "\n",
    "    mergeddf = None # empty variable\n",
    "    for stock in files_csv:\n",
    "        df = pd.read_csv(stock)\n",
    "\n",
    "        stock_name = stock.split('_')[0]\n",
    "        df = df[['Date','Volume']].copy() # using copy since copy enusures returning a new dataframe and not a view\n",
    "        df.rename(columns={'Volume':stock_name}, inplace=True)\n",
    "\n",
    "        if mergeddf is None:\n",
    "            mergeddf = df\n",
    "        else:\n",
    "            mergeddf = pd.merge(mergeddf, df, on='Date', how = 'outer')\n",
    "\n",
    "    # converting date from string to datetime:\n",
    "    mergeddf['Date'] = pd.to_datetime(mergeddf['Date'])\n",
    "    # sorting based on date to create sequence later:\n",
    "    mergeddf = mergeddf.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    return mergeddf\n",
    "\n",
    "volume_df = mergedfbydate_volume(files_csv)\n",
    "volume_df = volume_df.dropna()\n",
    "print(volume_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCDiWbcpNbJs",
    "outputId": "34deac75-6552-454e-ac46-9d4e87cab4eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "volume_df.info() # No null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6I1YsSbQNbJs",
    "outputId": "c65a4752-7c10-45e2-d1fe-90193f6b639c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "volume_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGxyKbfRcFl-",
    "outputId": "bbcc541f-a9a1-4673-aaa7-1aaa5cbe96cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frequency distribution of volumes\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for i,stock in enumerate(volume_df.columns[1:], start=1):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.histplot(volume_df[stock], bins=50, kde=True)\n",
    "    plt.title(f'{stock} stock distribution by volume')\n",
    "    plt.xlabel('Voulme')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q1g9Y0JcNW0",
    "outputId": "4e962c06-b2c4-43be-9eec-c1e30073fee5"
   },
   "outputs": [],
   "source": [
    "# Stock volume variation over time\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for stock in volume_df.columns[1:]:\n",
    "    plt.plot(volume_df['Date'], volume_df[stock], label= stock)\n",
    "plt.title('Stock volume variation over time')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO5frnjURc5i"
   },
   "source": [
    "#### **1.2.2** <font color =red> [3 marks] </font>\n",
    "Analyse correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE9ijOnmP94M",
    "outputId": "ba4732fb-bdb3-4c9d-8d04-9be0a7f9aae6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyse correlations\n",
    "\n",
    "# Corelating closing values of all the comapnies over time:\n",
    "plt.figure(figsize=(16,10))\n",
    "for stock in combined_df.columns[1:]:\n",
    "    plt.plot(combined_df['Date'], combined_df[stock], label= stock)\n",
    "plt.title('Stock closing value variation over time')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFRU8X2tNbJt",
    "outputId": "144e8417-61fe-4b1b-8037-fbf19d335a4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyzing correlation among the closing values in between the company stocks:\n",
    "\n",
    "# Drop Date for correlation\n",
    "corr_matrix = combined_df.drop(columns='Date').corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Between Closing Prices of Different Stocks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gcs6EZYKNbJt",
    "outputId": "ec5c22c4-5f8f-400a-d7ef-6c226b006483",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this instead of 'Close' we need to use the 'Open' column to make the combined dataframe. Lets do that first.\n",
    "# We will follow the same steps. Only difference will be the column name.\n",
    "\n",
    "# concatenating all the csv files into one dataframe and by date:\n",
    "\n",
    "def mergedfbydate_open(files_csv):\n",
    "\n",
    "    mergeddf = None # empty variable\n",
    "    for stock in files_csv:\n",
    "        df = pd.read_csv(stock)\n",
    "\n",
    "        stock_name = stock.split('_')[0]\n",
    "        df = df[['Date','Open']].copy() # using copy since copy enusures returning a new dataframe and not a view\n",
    "        df.rename(columns={'Open':stock_name}, inplace=True)\n",
    "\n",
    "        if mergeddf is None:\n",
    "            mergeddf = df\n",
    "        else:\n",
    "            mergeddf = pd.merge(mergeddf, df, on='Date', how = 'outer')\n",
    "\n",
    "    # converting date from string to datetime:\n",
    "    mergeddf['Date'] = pd.to_datetime(mergeddf['Date'])\n",
    "    # sorting based on date to create sequence later:\n",
    "    mergeddf = mergeddf.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    return mergeddf\n",
    "\n",
    "open_df = mergedfbydate_open(files_csv)\n",
    "open_df = volume_df.dropna()\n",
    "print(open_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85GB5bIBNbJt",
    "outputId": "a17d9b9f-8bda-47be-a757-89e01883918c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corelating opening values of all the comapnies over time:\n",
    "plt.figure(figsize=(16,10))\n",
    "for stock in combined_df.columns[1:]:\n",
    "    plt.plot(open_df['Date'], open_df[stock], label= stock)\n",
    "plt.title('Stock opening value variation over time')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Opening value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HTNACo2NbJt",
    "outputId": "d34b6cf1-829d-480d-ba2e-ae0b372b5042",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyzing correlation among the closing values in between the company stocks:\n",
    "\n",
    "# Drop Date for correlation\n",
    "corr_matrix = open_df.drop(columns='Date').corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='viridis', center=0)\n",
    "plt.title('Correlation Between Opening Prices of Different Stocks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kri8BVsNbJt",
    "outputId": "2e075921-c66f-4966-bc2a-23669ab8783e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this instead of 'Close' we need to use the 'Open' column to make the combined dataframe. Lets do that first.\n",
    "# We will follow the same steps. Only difference will be the column name.\n",
    "\n",
    "# concatenating all the csv files into one dataframe and by date:\n",
    "\n",
    "def mergedfbydate_high(files_csv):\n",
    "\n",
    "    mergeddf = None # empty variable\n",
    "    for stock in files_csv:\n",
    "        df = pd.read_csv(stock)\n",
    "\n",
    "        stock_name = stock.split('_')[0]\n",
    "        df = df[['Date','High']].copy() # using copy since copy enusures returning a new dataframe and not a view\n",
    "        df.rename(columns={'High':stock_name}, inplace=True)\n",
    "\n",
    "        if mergeddf is None:\n",
    "            mergeddf = df\n",
    "        else:\n",
    "            mergeddf = pd.merge(mergeddf, df, on='Date', how = 'outer')\n",
    "\n",
    "    # converting date from string to datetime:\n",
    "    mergeddf['Date'] = pd.to_datetime(mergeddf['Date'])\n",
    "    # sorting based on date to create sequence later:\n",
    "    mergeddf = mergeddf.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    return mergeddf\n",
    "\n",
    "high_df = mergedfbydate_high(files_csv)\n",
    "high_df = high_df.dropna()\n",
    "print(high_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1dby5rANbJu",
    "outputId": "4f2953d3-8833-44ef-c5ca-acf7c6ba44bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corelating high values of all the comapnies over time:\n",
    "plt.figure(figsize=(16,10))\n",
    "for stock in combined_df.columns[1:]:\n",
    "    plt.plot(open_df['Date'], open_df[stock], label= stock)\n",
    "plt.title('Stock high value variation over time')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('High value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmlwltKPNbJy",
    "outputId": "1f43fd9a-d45e-499c-8a0b-eba7a46a29b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyzing correlation among the closing values in between the company stocks:\n",
    "\n",
    "# Drop Date for correlation\n",
    "corr_matrix = high_df.drop(columns='Date').corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='plasma', center=0)\n",
    "plt.title('Correlation Between High Prices of Different Stocks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrbVbnzHNbJy",
    "outputId": "383a32ae-beb3-4e8e-aad2-5dcc43cb4a97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this instead of 'Close' we need to use the 'Open' column to make the combined dataframe. Lets do that first.\n",
    "# We will follow the same steps. Only difference will be the column name.\n",
    "\n",
    "# concatenating all the csv files into one dataframe and by date:\n",
    "\n",
    "def mergedfbydate_low(files_csv):\n",
    "\n",
    "    mergeddf = None # empty variable\n",
    "    for stock in files_csv:\n",
    "        df = pd.read_csv(stock)\n",
    "\n",
    "        stock_name = stock.split('_')[0]\n",
    "        df = df[['Date','Low']].copy() # using copy since copy enusures returning a new dataframe and not a view\n",
    "        df.rename(columns={'Low':stock_name}, inplace=True)\n",
    "\n",
    "        if mergeddf is None:\n",
    "            mergeddf = df\n",
    "        else:\n",
    "            mergeddf = pd.merge(mergeddf, df, on='Date', how = 'outer')\n",
    "\n",
    "    # converting date from string to datetime:\n",
    "    mergeddf['Date'] = pd.to_datetime(mergeddf['Date'])\n",
    "    # sorting based on date to create sequence later:\n",
    "    mergeddf = mergeddf.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    return mergeddf\n",
    "\n",
    "low_df = mergedfbydate_low(files_csv)\n",
    "low_df = low_df.dropna()\n",
    "print(low_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVQu6s6aNbJz",
    "outputId": "63944a5a-c212-431c-e021-55670c9a3b91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corelating high values of all the comapnies over time:\n",
    "plt.figure(figsize=(16,10))\n",
    "for stock in low_df.columns[1:]:\n",
    "    plt.plot(low_df['Date'], low_df[stock], label= stock)\n",
    "plt.title('Stock low value variation over time')\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Low value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYTelhDBNbJz",
    "outputId": "c45da548-72b5-49a9-d0c0-ab9f40ba9355",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyzing correlation among the closing values in between the company stocks:\n",
    "\n",
    "# Drop Date for correlation\n",
    "corr_matrix = low_df.drop(columns='Date').corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='cividis', center=0)\n",
    "plt.title('Correlation Between low Prices of Different Stocks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1dHdCDQl-1K"
   },
   "source": [
    "### **1.3 Data Processing** <font color =red> [13 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjEYTmOopW4"
   },
   "source": [
    "Next, we need to process the data so that it is ready to be used in recurrent neural networks. You know RNNs are suitable to work with sequential data where patterns repeat at regular intervals.\n",
    "\n",
    "For this, we need to execute the following steps:\n",
    "1. Create windows from the master data frame and obtain windowed `X` and corresponding windowed `y` values\n",
    "2. Perform train-test split on the windowed data\n",
    "3. Scale the data sets in an appropriate manner\n",
    "\n",
    "We will define functions for the above steps that finally return training and testing data sets that are ready to be used in recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTLu14Wuid5n"
   },
   "source": [
    "**Hint:** If we use a window of size 3, in the first window, the rows `[0, 1, 2]` will be present and will be used to predict the value of `CloseAMZN` in row `3`. In the second window, rows `[1, 2, 3]` will be used to predict `CloseAMZN` in row `4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mulMxtd6fsFR"
   },
   "source": [
    "#### **1.3.1** <font color =red> [3 marks] </font>\n",
    "Create a function that returns the windowed `X` and `y` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXQOh3iP6W7L"
   },
   "source": [
    "From the main DataFrame, this function will create windowed DataFrames, and store those as a list of DataFrames.\n",
    "\n",
    "Controllable parameters will be window size, step size (window stride length) and target names as a list of the names of stocks whose closing values we wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n1M1-4fvq8i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function that divides the data into windows and generates target variable values for each window\n",
    "def create_windows(series, window_size=30, horizon=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size - horizon + 1):\n",
    "        window = series[i : i + window_size]\n",
    "        target = series[i + window_size : i + window_size + horizon]\n",
    "        X.append(window)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw5JL8tgmwc-"
   },
   "source": [
    "#### **1.3.2** <font color =red> [3 marks] </font>\n",
    "Create a function to scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBaJkaIx_1Vg"
   },
   "source": [
    "Define a function that will scale the data.\n",
    "\n",
    "For scaling, we have to look at the whole length of data to find max/min values or standard deviations and means. If we scale the whole data at once, this will lead to data leakage in the windows. This is not necessarily a problem if the model is trained on the complete data with cross-validation.\n",
    "\n",
    "One way to scale when dealing with windowed data is to use the `partial_fit()` method.\n",
    "```\n",
    "scaler.partial_fit(window)\n",
    "scaler.transform(window)\n",
    "```\n",
    "You may use any other suitable way to scale the data properly. Arrive at a reasonable way to scale your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9586NZptrAi"
   },
   "outputs": [],
   "source": [
    "# Define a function that scales the windowed data\n",
    "# The function takes in the windowed data sets and returns the scaled windows\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_series_partial(series, window_size=30):\n",
    "    scaler = MinMaxScaler()\n",
    "    reshaped_series = series.reshape(-1, 1)\n",
    "    for i in range(len(reshaped_series) - window_size + 1):\n",
    "        window = reshaped_series[i : i + window_size]\n",
    "        scaler.partial_fit(window)\n",
    "    scaled = scaler.transform(reshaped_series).flatten()\n",
    "    return scaled, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3HhUS59DaCQ"
   },
   "source": [
    "Next, define the main function that will call the windowing and scaling helper functions.\n",
    "\n",
    "The input parameters for this function are:\n",
    "- The joined master data set\n",
    "- The names of the stocks that we wish to predict the *Close* prices for\n",
    "- The window size\n",
    "- The window stride\n",
    "- The train-test split ratio\n",
    "\n",
    "The outputs from this function are the scaled dataframes:\n",
    "- *X_train*\n",
    "- *y_train*\n",
    "- *X_test*\n",
    "- *y_test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tND37dF1ksmy"
   },
   "source": [
    "#### **1.3.3** <font color =red> [3 marks] </font>\n",
    "Define a function to create windows of `window_size` and split the windowed data in to training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3acwSBoArWU"
   },
   "source": [
    "The function can take arguments such as list of target names, window size, window stride and split ratio. Use the windowing function here to make windows in the data and then perform scaling and train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvWRH5J4vq8l"
   },
   "outputs": [],
   "source": [
    "# Define a function to create input and output data points from the master DataFrame\n",
    "\n",
    "def split_train_val(X, y, split_ratio=0.8):\n",
    "    split_index = int(len(X) * split_ratio)\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_val, y_val = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5PZw7SHGUdx"
   },
   "source": [
    "We can now use these helper functions to create our training and testing data sets. But first we need to decide on a length of windows. As we are doing time series prediction, we want to pick a sequence that shows some repetition of patterns.\n",
    "\n",
    "For selecting a good sequence length, some business understanding will help us. In financial scenarios, we can either work with business days, weeks (which comprise of 5 working days), months, or quarters (comprising of 13 business weeks). Try looking for some patterns for these periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mihb_duwxWeC"
   },
   "source": [
    "#### **1.3.4** <font color =red> [2 marks] </font>\n",
    "Identify an appropriate window size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-LrkMVjgT-"
   },
   "source": [
    "For this, you can use plots to see how target variable is varying with time. Try dividing it into parts by weeks/months/quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhhbomJIrlK_"
   },
   "outputs": [],
   "source": [
    "# Checking for patterns in different sequence lengths\n",
    "\n",
    "# Plot segments of a stock time series by month ('M'), week ('W'), or quarter ('Q').\n",
    "# Helps visualize trends and identify a good window size.\n",
    "\n",
    "def plot_by_time_slices(df, stock_name, freq='M'):\n",
    "    df_temp = df[['Date', stock_name]].copy()\n",
    "    df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "    df_temp.set_index('Date', inplace=True)\n",
    "    grouped = df_temp.groupby(pd.Grouper(freq=freq))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "        plt.plot(group.index, group[stock_name], label=str(name.date()))\n",
    "    plt.title(f\"{stock_name} by {freq} slices\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    #plt.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YbeNZSwNbJ1",
    "outputId": "9e4f39d1-69b1-4353-ff68-f8dccaf813f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_by_time_slices(combined_df, 'AMZN', 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93o7Dq-lNbJ1"
   },
   "outputs": [],
   "source": [
    "# Lets take a window size of 30 days. 30 time steps --> since each observation is on next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZpKaENfyblo"
   },
   "source": [
    "#### **1.3.5** <font color =red> [2 marks] </font>\n",
    "Call the functions to create testing and training instances of predictor and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sFdDgH0vq8o"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using decided window size and window stride\n",
    "\n",
    "def prepare_stock_data(df, stock_name, window_size=30, horizon=1):\n",
    "    series = df[stock_name].values.astype(np.float32)\n",
    "    scaled_series, scaler = scale_series_partial(series, window_size)\n",
    "    X, y = create_windows(scaled_series, window_size, horizon)\n",
    "    return split_train_val(X, y), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoRLGqp-6aHP"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "\n",
    "# Will do this individually after creating model for each stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51IV5zmjBf-w"
   },
   "source": [
    "**Check if the training and testing datasets are in the proper format to feed into neural networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViVKgkrwvq8r"
   },
   "outputs": [],
   "source": [
    "# Check if the datasets are compatible inputs to neural networks\n",
    "\n",
    "# Will do this individually after creating model for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUu4LLe-NbJ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNNSAT6YfO3z"
   },
   "source": [
    "## **2 RNN Models** <font color =red> [20 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqHs0PjzwCve"
   },
   "source": [
    "In this section, we will:\n",
    "- Define a function that creates a simple RNN\n",
    "- Tune the RNN for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekq-LY1p86NI"
   },
   "source": [
    "### **2.1 Simple RNN Model** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdzl5ojcyDX7"
   },
   "source": [
    "#### **2.1.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds a simple RNN model based on the layer configuration provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owGKpgMxRtk2"
   },
   "outputs": [],
   "source": [
    "# Create a function that creates a simple RNN model according to the model configuration arguments\n",
    "\n",
    "def build_simple_rnn(window_size, horizon):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "    model = Sequential([\n",
    "        SimpleRNN(32, input_shape=(window_size, 1)),\n",
    "        Dense(horizon)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8rcwYQpNbJ2",
    "outputId": "d9c7f8ba-2bf8-4c6b-bf1a-bd44e60a19c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'MSFT'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='SimpleRNN', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"RNN_MSFT.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - RNN Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC6zcJcNNbJ2",
    "outputId": "2279e468-a885-4db5-cfd8-bcce4ccfc0d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'IBM'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='SimpleRNN', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"RNN_IBM.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - RNN Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHA7soHyNbJ2",
    "outputId": "a37f09c9-9c84-4c21-a63f-970441e1c077",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'GOOGL'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='SimpleRNN', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"RNN_GOOGL.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - RNN Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx-hyYoLNbJ3",
    "outputId": "452af694-e39b-4fa8-e69a-c7a5623e6298",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'AMZN'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='SimpleRNN', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"RNN_AMZN.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - RNN Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJV4j2zeNbJ3",
    "outputId": "86328966-7aae-4063-92b3-dbae63c86f76",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'MSFT'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='LSTM', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"model_MSFT.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - LSTM Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mHTPT1lNbJ3",
    "outputId": "369c40ba-b504-4374-9de2-af6857a4ae53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'IBM'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='LSTM', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"model_IBM.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - LSTM Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGH6nWk9NbJ3",
    "outputId": "13032136-8427-4713-f111-9601a182bdd0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'GOOGL'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='LSTM', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"model_GOOGL.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - LSTM Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eRs8xAENbJ3",
    "outputId": "74b2c1bd-dd3f-4b1b-b751-fa289ffafa9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'AMZN'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='LSTM', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"model_AMZN.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - LSTM Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xDFvOXfO31"
   },
   "source": [
    "#### **2.1.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGRsV3GefO31",
    "outputId": "972cc966-372f-40ba-867f-f310ab9c9436",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "\n",
    "# Lets use keras tuner for hyperparameter tunning:\n",
    "\n",
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x7w0b0SNbJ3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
    "\n",
    "def model_builder(hp, model_type='LSTM'):\n",
    "    model = Sequential()\n",
    "\n",
    "    units = hp.Int(\"units\", min_value=32, max_value=128, step=32)\n",
    "    dropout = hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(units, input_shape=(window_size, 1)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(units, input_shape=(window_size, 1)))\n",
    "    elif model_type == 'SimpleRNN':\n",
    "        model.add(SimpleRNN(units, input_shape=(window_size, 1)))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type\")\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(forecast_horizon))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G84ZW4SFNbJ4",
    "outputId": "0cd59cc0-875a-4f30-e3b7-4917456392aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    lambda hp: model_builder(hp, model_type='SimpleRNN'),  # or 'LSTM', 'SimpleRNN'\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"tune_rnn_msft\"\n",
    ")\n",
    "\n",
    "# Train with your preprocessed data\n",
    "\n",
    "# For MSFT\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, \"MSFT\", 30, 1)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHuJ61iYfO31",
    "outputId": "57ea4559-47bf-498b-a010-d77158229dfe"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration based on evaluation metrics\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "# Optionally save it\n",
    "best_model.save(\"best_RNN_MSFT.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_W0Z-m_NbJ4",
    "outputId": "f4b1ef2b-11ee-45eb-9fcf-ca06aa6bbed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    lambda hp: model_builder(hp, model_type='SimpleRNN'),  # or 'LSTM', 'SimpleRNN'\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"tune_rnn_IBM\"\n",
    ")\n",
    "\n",
    "# Train with your preprocessed data\n",
    "\n",
    "# For IBM\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, \"IBM\", 30, 1)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlKjM4N_NbJ4",
    "outputId": "29d6f8a5-b9a8-4f31-e9e5-b85c30a1dd68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the best configuration based on evaluation metrics\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "# Optionally save it\n",
    "best_model.save(\"best_RNN_IBM.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hzp0JEjqNbJ4",
    "outputId": "98f7cdd4-4ee1-4e0e-8aaa-1b7832a9b623",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For RNN_IBM\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72fOcDy_NbJ4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similarly other models can also be tunned using 'keras tunner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8krOJTq5_fM0"
   },
   "source": [
    "#### **2.1.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal Simple RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLQPoIQCfO32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"best_RNN_MSFT.h5\")\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, \"MSFT\", window_size=30, horizon=1)\n",
    "y_pred = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QceWx7D78RH"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK1yY499Ynpq"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUfu3167Rqx"
   },
   "source": [
    "It is worth noting that every training session for a neural network is unique. So, the results may vary slightly each time you retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHlPLvDcfO32"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLmMGXvyvq9V"
   },
   "source": [
    "### **2.2 Advanced RNN Models** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCaJYIHzwzP9"
   },
   "source": [
    "In this section, we will:\n",
    "- Create an LSTM or a GRU network\n",
    "- Tune the network for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6KBxY4Jasm5"
   },
   "source": [
    "#### **2.2.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds an advanced RNN model with tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g1yDzllvq9W"
   },
   "outputs": [],
   "source": [
    "# # Define a function to create a model and specify default values for hyperparameters\n",
    "\n",
    "def build_advanced_rnn(window_size, horizon, model_type='LSTM', units=64, dropout=0.2):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "\n",
    "    model = Sequential()\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(units, input_shape=(window_size, 1)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(units, input_shape=(window_size, 1)))\n",
    "    elif model_type == 'SimpleRNN':\n",
    "        model.add(SimpleRNN(units, input_shape=(window_size, 1)))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type. Choose from 'LSTM', 'GRU', 'SimpleRNN'\")\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(horizon))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Bz0scYqNbJ5",
    "outputId": "31d8696a-a715-4411-daa1-69b3ae3c319d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'MSFT'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='GRU', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"GRU_MSFT.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - GRU Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQg8hObdNbJ5",
    "outputId": "1d9656c8-da29-487c-9e2e-3d24bb4483ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'IBM'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='GRU', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"GRU_IBM.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - GRU Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2JiZfBsNbJ5",
    "outputId": "6d26bff6-cd89-4b2a-9a54-db56bb504109",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'GOOGL'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='GRU', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"GRU_GOOGL.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - GRU Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aTO6hnRNbJ6",
    "outputId": "34767ac7-c250-416a-e7e1-c0b78596df6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Setup: Choose parameters and stock\n",
    "stock_name = 'AMZN'\n",
    "window_size = 30\n",
    "forecast_horizon = 1\n",
    "\n",
    "# 1. Plot the raw series\n",
    "# plot_series(combined_df[stock_name], stock_name)\n",
    "\n",
    "# 2. Prepare the data (scaling + windowing + split)\n",
    "(X_train, y_train, X_val, y_val), scaler = prepare_stock_data(combined_df, stock_name, window_size, forecast_horizon)\n",
    "\n",
    "# 3. Reshape X for RNN input\n",
    "X_train = X_train.reshape(-1, window_size, 1)\n",
    "X_val = X_val.reshape(-1, window_size, 1)\n",
    "\n",
    "# 4. Build the model\n",
    "model = build_advanced_rnn(window_size, forecast_horizon, model_type='GRU', units=64)\n",
    "# Save the model to disk\n",
    "model.save(\"GRU_AMZN.h5\")\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# 6. Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 7. Inverse scale predictions and ground truth\n",
    "y_val_inv = scaler.inverse_transform(y_val)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 8. Plot predictions vs true values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_val_inv[:100], label='True')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted', linestyle='--')\n",
    "plt.title(f'{stock_name} - GRU Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0lvqIJ2vq9b"
   },
   "source": [
    "#### **2.2.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWRBShecvq9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration\n",
    "\n",
    "# Done earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n5OTBdSvq9t"
   },
   "source": [
    "#### **2.2.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uyN6vgzvq9w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "# Done earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x9njgzwvq92"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "# Done earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36Hy584ffhsS"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31CshI-SfhsS"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "# Done earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiS0WMzWNbJ6"
   },
   "source": [
    "### Checking the best models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl_85qE5NbJ6",
    "outputId": "f0611b99-cb0d-4894-eb31-8d17c81c08a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 1: Load saved model\n",
    "model = load_model(\"best_RNN_MSFT.h5\")\n",
    "\n",
    "# Step 2: Define global scaling function\n",
    "def scale_series_global(series):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "    return scaled, scaler\n",
    "\n",
    "# Step 3: Create windows\n",
    "def create_windows(series, window_size=30, horizon=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size - horizon + 1):\n",
    "        window = series[i : i + window_size]\n",
    "        target = series[i + window_size : i + window_size + horizon]\n",
    "        X.append(window)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Prepare test/validation data\n",
    "def prepare_stock_data(df, stock_name, window_size=30, horizon=1, split_ratio=0.8):\n",
    "    series = df[stock_name].values.astype(np.float32)\n",
    "    scaled_series, scaler = scale_series_global(series)\n",
    "    X, y = create_windows(scaled_series, window_size, horizon)\n",
    "    split_index = int(len(X) * split_ratio)\n",
    "    X_val = X[split_index:]\n",
    "    y_val = y[split_index:]\n",
    "    return X_val, y_val, scaler\n",
    "\n",
    "# Step 5: Load and prepare data\n",
    "window_size = 30\n",
    "horizon = 1\n",
    "\n",
    "X_val, y_val, scaler = prepare_stock_data(combined_df, 'AMZN', window_size, horizon)\n",
    "\n",
    "# Reshape X for prediction\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Step 7: Inverse scale for real price values\n",
    "y_val_inv = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "mae = mean_absolute_error(y_val_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_inv, y_pred_inv))\n",
    "r2 = r2_score(y_val_inv, y_pred_inv)\n",
    "\n",
    "print(f\"\\n Evaluation Metrics:\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")\n",
    "\n",
    "# Step 9: Plot Predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_val_inv[:100], label='True Price', linewidth=2)\n",
    "plt.plot(y_pred_inv[:100], label='Predicted Price', linestyle='--')\n",
    "plt.title(\"MSFT Stock Price Prediction vs Actual\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc4RBydDNbJ7",
    "outputId": "ac16758e-5b66-43d1-c93a-64adc031b7f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 1: Load saved model\n",
    "model = load_model(\"best_RNN_IBM.h5\")\n",
    "\n",
    "# Step 2: Define global scaling function\n",
    "def scale_series_global(series):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "    return scaled, scaler\n",
    "\n",
    "# Step 3: Create windows\n",
    "def create_windows(series, window_size=30, horizon=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size - horizon + 1):\n",
    "        window = series[i : i + window_size]\n",
    "        target = series[i + window_size : i + window_size + horizon]\n",
    "        X.append(window)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Prepare test/validation data\n",
    "def prepare_stock_data(df, stock_name, window_size=30, horizon=1, split_ratio=0.8):\n",
    "    series = df[stock_name].values.astype(np.float32)\n",
    "    scaled_series, scaler = scale_series_global(series)\n",
    "    X, y = create_windows(scaled_series, window_size, horizon)\n",
    "    split_index = int(len(X) * split_ratio)\n",
    "    X_val = X[split_index:]\n",
    "    y_val = y[split_index:]\n",
    "    return X_val, y_val, scaler\n",
    "\n",
    "# Step 5: Load and prepare data\n",
    "window_size = 30\n",
    "horizon = 1\n",
    "\n",
    "X_val, y_val, scaler = prepare_stock_data(combined_df, 'AMZN', window_size, horizon)\n",
    "\n",
    "# Reshape X for prediction\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Step 7: Inverse scale for real price values\n",
    "y_val_inv = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "mae = mean_absolute_error(y_val_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_inv, y_pred_inv))\n",
    "r2 = r2_score(y_val_inv, y_pred_inv)\n",
    "\n",
    "print(f\"\\n Evaluation Metrics:\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")\n",
    "\n",
    "# Step 9: Plot Predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_val_inv[:100], label='True Price', linewidth=2)\n",
    "plt.plot(y_pred_inv[:100], label='Predicted Price', linestyle='--')\n",
    "plt.title(\"MSFT Stock Price Prediction vs Actual\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uU9v5GeNbJ7"
   },
   "outputs": [],
   "source": [
    "# Similarly other models can be tunned and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrLtLyRTNbJ7"
   },
   "source": [
    "**Similarly other models can be tunned and evaluated. Not all models are tunned and checked in this notebook but the\n",
    "code cells for the task have been included.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c97jLdcgNbJ7"
   },
   "source": [
    "### Forecasting Shown ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB9O88uxNbJ7",
    "outputId": "430a5e62-0924-41f8-f1f1-d374a792eadc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forecasting (Example) --> Only one shown\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"best_RNN_IBM.h5\")\n",
    "\n",
    "# Settings\n",
    "stock_name = \"IBM\"\n",
    "window_size = 30\n",
    "forecast_horizon = 100  # number of days to forecast\n",
    "\n",
    "# Prepare the series and scaler\n",
    "series = combined_df[stock_name].values.astype(np.float32)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_series = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Get last window\n",
    "last_window = scaled_series[-window_size:]\n",
    "\n",
    "# Forecasting loop\n",
    "forecast_scaled = []\n",
    "current_input = last_window.copy()\n",
    "\n",
    "for _ in range(forecast_horizon):\n",
    "    input_reshaped = current_input.reshape((1, window_size, 1))\n",
    "    pred_scaled = model.predict(input_reshaped, verbose=0)[0][0]  # one-step prediction\n",
    "    forecast_scaled.append(pred_scaled)\n",
    "\n",
    "    # Slide the window\n",
    "    current_input = np.append(current_input[1:], pred_scaled)\n",
    "\n",
    "# Inverse scale predictions\n",
    "forecast = scaler.inverse_transform(np.array(forecast_scaled).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Generate forecast dates\n",
    "last_date = combined_df['Date'].iloc[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='B')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(combined_df['Date'], combined_df[stock_name], label='Historical')\n",
    "plt.plot(future_dates, forecast, '-', label='Forecast')  # <<== fixed here\n",
    "plt.axvline(x=last_date, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(f\"{stock_name} - Next {forecast_horizon} Day Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EStsPXCYf3_q"
   },
   "source": [
    "## **3 Predicting Multiple Target Variables** <font color =red> [OPTIONAL] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKaiTII-23Aq"
   },
   "source": [
    "In this section, we will use recurrent neural networks to predict stock prices for more than one company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taBw30-HLTWV"
   },
   "source": [
    "### **3.1 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9aHdR6EgVqT"
   },
   "source": [
    "#### **3.1.1**\n",
    "Create testing and training instances for multiple target features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua3P_ySxgq9Z"
   },
   "source": [
    "You can take the closing price of all four companies to predict here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQEWm-m129Rw"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using a window size of 65, a window stride of 5 and a test size of 20%\n",
    "# Specify the list of stock names whose 'Close' values you wish to predict using the 'target_names' parameter\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_WWJKvsLlAA"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kUyl2U9LnpK"
   },
   "source": [
    "### **3.2 Run RNN Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2HTWIcWhLE0"
   },
   "source": [
    "#### **3.2.1**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Simple RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhjXH72pMI4t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYXngNUmMI4u"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vyWykGBMI4y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j63wDQkMcNB"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QbM-hR6h-SV"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnwOVGv5MjXi"
   },
   "source": [
    "#### **3.2.2**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Advanced RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQRqZZbEIrh9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of advanced RNN\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZswZ55JIrh-"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fh-2tBTNWXI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y7m-nziNWXK"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OMsd4hHicRO"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n",
    "# Not attempted due to time constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YItIF9_SmeCN"
   },
   "source": [
    "## **4 Conclusion** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGwEyQn1meCN"
   },
   "source": [
    "### **4.1 Conclusion and insights** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqBneTdDaVYw"
   },
   "source": [
    "#### **4.1.1** <font color =red> [5 marks] </font>\n",
    "Conclude with the insights drawn and final outcomes and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b91t0L39O_ir"
   },
   "source": [
    "# Stock Price Prediction Using RNNs\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this project is to try and predict the stock prices using historical data from four technology companies: IBM, Google (GOOGL), Amazon (AMZN), and Microsoft (MSFT). By using data from multiple companies in the same sector, we aim to capture broader market sentiment and potentially improve prediction accuracy.\n",
    "\n",
    "The problem statement can be summarized as follows: Given the stock prices of Amazon, Google, IBM, and Microsoft for a set number of days, predict the stock price of these companies after that window.\n",
    "\n",
    "## Business Value\n",
    "\n",
    "Stock market data is inherently sequential, making Recurrent Neural Networks (RNNs) well-suited for this type of analysis. Tracking metrics like open, close, high, low prices, and volume provides a rich time series dataset. Identifying patterns within this data is not only academically interesting but also holds significant financial value, as accurate predictions can lead to profitable investment strategies.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The project utilizes four CSV files, one for each stock (AMZN, GOOGL, IBM, MSFT). These files contain historical data from January 1, 2006, to January 1, 2018, sourced from NYSE and NASDAQ. Each file includes the following columns:\n",
    "\n",
    "- `Date`: The date of the record.\n",
    "- `Open`: The stock price at market open.\n",
    "- `High`: The highest stock price during the day.\n",
    "- `Low`: The lowest stock price during the day.\n",
    "- `Close`: The stock price at market close.\n",
    "- `Volume`: The total number of shares traded.\n",
    "- `Name`: The stock ticker symbol.\n",
    "\n",
    "Each dataset contains 3019 records.\n",
    "\n",
    "## 1 Data Loading and Preparation\n",
    "\n",
    "This section covers loading the individual company stock data, aggregating it into a single DataFrame, handling missing values, and performing initial data analysis and visualization.\n",
    "\n",
    "### 1.1 Data Aggregation\n",
    "\n",
    "The data from the four individual CSV files is combined into a single DataFrame. This involves:\n",
    "- Reading each CSV into a DataFrame.\n",
    "- Selecting and renaming the `Close` price column with the company name.\n",
    "- Merging the DataFrames based on the `Date` column.\n",
    "- Converting the `Date` column to datetime objects and sorting the DataFrame.\n",
    "\n",
    "Missing values are identified and handled by dropping rows with any `NaN` values.\n",
    "\n",
    "### 1.2 Analysis and Visualisation\n",
    "\n",
    "Exploratory data analysis is performed to understand the characteristics of the data.\n",
    "- **Volume Analysis:** The frequency distribution of stock volumes for each company is visualized using histograms. A line plot shows the variation of trading volume over time for each stock.\n",
    "- **Correlation Analysis:** Line plots visualize the variation of closing, opening, high, and low prices over time for all stocks. Heatmaps are used to visualize the correlation matrix of closing, opening, high, and low prices among the four stocks. This helps in understanding the relationships between the stock movements.\n",
    "\n",
    "### 1.3 Data Processing\n",
    "\n",
    "The data is preprocessed to be suitable for RNN models, which work with sequential data.\n",
    "- **Windowing:** A `create_windows` function is defined to create input sequences (windows) and corresponding target values from the time series data. A window size of 30 days is chosen after observing potential patterns in monthly slices of the data.\n",
    "- **Scaling:** A `scale_series_partial` function (though global scaling was eventually used for final evaluation) is initially considered for scaling data within windows to prevent data leakage.\n",
    "- **Train-Test Split:** A `split_train_val` function splits the windowed data into training and validation sets.\n",
    "\n",
    "These functions are combined in `prepare_stock_data` to generate the final scaled and windowed training and validation datasets.\n",
    "\n",
    "## 2 RNN Models\n",
    "\n",
    "This section focuses on building, tuning, and evaluating different RNN models for stock price prediction.\n",
    "\n",
    "### 2.1 Simple RNN Model\n",
    "\n",
    "A basic Simple RNN model is built using a single `SimpleRNN` layer followed by a `Dense` layer.\n",
    "\n",
    "Hyperparameter tuning is performed using KerasTuner `RandomSearch` to find the optimal number of units and dropout rate for the Simple RNN model for each stock.\n",
    "\n",
    "The best performing Simple RNN model for each stock is then loaded and evaluated on the validation set. Predictions are inverse-scaled to compare with the actual stock prices.\n",
    "\n",
    "### 2.2 Advanced RNN Models\n",
    "\n",
    "This section explores more advanced RNN architectures like LSTM and GRU.\n",
    "\n",
    "A `build_advanced_rnn` function is created to build models using `LSTM`, `GRU`, or `SimpleRNN` layers, allowing for tuning of units and dropout.\n",
    "\n",
    "LSTM and GRU models are built and trained for each stock. Predictions are made on the validation set and inverse-scaled for comparison and evaluation.\n",
    "\n",
    "Hyperparameter tuning using KerasTuner can also be applied to LSTM and GRU models in a similar fashion to the Simple RNN.\n",
    "\n",
    "The optimal models found through tuning (e.g., `best_RNN_MSFT.h5`, `best_RNN_IBM.h5`) are loaded, and their performance is evaluated on the validation data using metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R²). Plots showing the comparison of actual and predicted prices are generated.\n",
    "\n",
    "## 3 Predicting Multiple Target Variables (Optional)\n",
    "\n",
    "This section is outlined for exploring the possibility of predicting the closing prices of all four companies simultaneously using a multi-output RNN model. Due to time constraints, this part of the project was not completed, but the steps would involve:\n",
    "- Preparing data with multiple target columns.\n",
    "- Building RNN models with multiple output neurons.\n",
    "- Tuning hyperparameters for the multi-output models.\n",
    "- Evaluating the performance across all predicted stocks.\n",
    "\n",
    "## 4 Conclusion\n",
    "\n",
    "### 4.1 Conclusion and insights\n",
    "\n",
    "This project successfully demonstrates the application of Recurrent Neural Networks (Simple RNN, LSTM, and GRU) for predicting stock prices using historical data from multiple technology companies.\n",
    "\n",
    "The data aggregation and preprocessing steps, including handling missing values and creating windowed sequences, were crucial for preparing the time series data for RNNs. The exploratory data analysis provided valuable insights into the data's characteristics and the correlations between the stock prices of different companies. The strong positive correlation observed among the stock prices suggests that using data from multiple companies in the same sector is a reasonable approach and could potentially help the models capture sector-wide trends.\n",
    "\n",
    "Hyperparameter tuning using KerasTuner helped identify better network configurations for the Simple RNN models, leading to improved performance.\n",
    "\n",
    "While the Simple RNN models showed some ability to capture the general trend of the stock prices, the plots and evaluation metrics (MAE, RMSE, R²) indicate that the predictions are not perfectly aligned with the actual values. Factors like market volatility, unforeseen events, and the inherent complexity of financial markets make accurate stock price prediction a challenging task. The performance metrics provide a quantitative measure of the model's accuracy, with lower MAE and RMSE indicating better predictions and a higher R² indicating a better fit to the data.\n",
    "\n",
    "The advanced RNN models (LSTM and GRU) were also implemented and showed similar performance characteristics, suggesting that for this specific dataset and problem setup, the added complexity of LSTM or GRU might not yield significantly better results compared to a tuned Simple RNN, although further extensive tuning could explore this further.\n",
    "\n",
    "The forecasting example illustrates how a trained model can be used to predict future stock prices for a specified horizon. While this provides a glimpse into potential future movements, it's important to remember that these are predictions based on past patterns and do not guarantee future outcomes.\n",
    "\n",
    "**In summary,** this project provides a foundational approach to stock price prediction using RNNs. The results highlight the potential of these models for time series forecasting in finance, while also emphasizing the difficulty of achieving high accuracy in a volatile market. Future work could explore more complex architectures, incorporating external factors (e.g., news sentiment, economic indicators), and employing more advanced feature engineering and scaling techniques to potentially improve predictive performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
